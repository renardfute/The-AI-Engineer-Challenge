{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQt-gyAYUbm3"
   },
   "source": [
    "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PInACkIWUhOd"
   },
   "source": [
    "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecnJouXnUgKv",
    "outputId": "c6c25850-395d-4cbf-9d26-bfe9253d1711"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1pOrbwSU5H_"
   },
   "source": [
    "### Our First Prompt\n",
    "\n",
    "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
    "\n",
    "Let's create a `ChatCompletion` model to kick things off!\n",
    "\n",
    "There are three \"roles\" available to use:\n",
    "\n",
    "- `developer`\n",
    "- `assistant`\n",
    "- `user`\n",
    "\n",
    "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
    "\n",
    "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
    "\n",
    "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iy_LEPNEMVvC"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofMwuUQOU4sf",
    "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-C0Zemtc8LrALLPpg1GdZ72f6kXwLS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular tools in the AI/LLM ecosystem, but they serve different purposes and have distinct features. Here's an overview of their differences:\\n\\n**1. Purpose and Use Cases:**\\n\\n- **LangChain:**\\n  - Primarily a framework for building, managing, and deploying applications that leverage large language models (LLMs).\\n  - Focuses on creating chains, prompts, and workflows to perform complex NLP tasks like chatbots, question-answering systems, or automation pipelines.\\n  - Provides abstractions for chaining together models, prompts, memory, and tools.\\n\\n- **LlamaIndex (GPT Index):**\\n  - Designed to facilitate the integration of external data sources into LLM workflows.\\n  - Focuses on indexing, storing, and retrieving large amounts of unstructured data (documents, PDFs, databases) to enable efficient question answering and data retrieval with LLMs.\\n  - Acts as a data framework that creates “indices” over user data for quick retrieval.\\n\\n**2. Core Functionalities:**\\n\\n- **LangChain:**\\n  - Provides components like prompts, chains, memory, and agents.\\n  - Supports various LLM providers (OpenAI, AI21, etc.).\\n  - Enables complex multi-step workflows, such as summarization, translation, or reasoning.\\n  - Emphasizes modularity and extensibility in building applications.\\n\\n- **LlamaIndex:**\\n  - Offers tools to load, process, and index large datasets.\\n  - Implements retrieval-augmented generation (RAG) workflows to answer questions based on indexed data.\\n  - Supports multiple index types (tree, list, GPT-based, etc.).\\n  - Focuses on making it easy to incorporate custom data into LLM-based solutions.\\n\\n**3. Integration and Ecosystem:**\\n\\n- **LangChain:**\\n  - Has a broad ecosystem with integrations to various LLM providers, tools, and APIs.\\n  - Used with few external data management tools but can be combined with them.\\n\\n- **LlamaIndex:**\\n  - Designed to work seamlessly with document stores, databases, and file systems.\\n  - Can be integrated into LangChain workflows for enhanced data retrieval.\\n\\n**4. Typical Use Cases:**\\n\\n- **LangChain:**\\n  - Building chatbots, virtual assistants, summarizers, or workflows that require complex logic.\\n  - Automating tasks with LLMs in a controlled, step-by-step manner.\\n\\n- **LlamaIndex:**\\n  - Creating knowledge bases by indexing documents.\\n  - Answering questions based on large unstructured datasets.\\n  - Facilitating retrieval-augmented generation for data-driven applications.\\n\\n---\\n\\n### Summary Table\\n\\n| Aspect                  | LangChain                                         | LlamaIndex (GPT Index)                           |\\n|-------------------------|---------------------------------------------------|-------------------------------------------------|\\n| Main Purpose            | Building LLM-powered applications and workflows   | Indexing and retrieving data for LLM usage   |\\n| Focus                   | Chains, prompts, tools, memory, and workflows    | Data indexing, retrieval, and augmentation    |\\n| Typical Use Cases       | Chatbots, automation, NLP pipelines               | Knowledge bases, question answering from datasets |\\n| Integration             | Broad LLM provider support, modular components   | Data ingestion, indexing, retrieval           |\\n\\n---\\n\\n**In short:**  \\n- Use **LangChain** if you need to orchestrate complex LLM workflows, chain multiple models, or build interactive applications.  \\n- Use **LlamaIndex** if you want to efficiently index, search, and retrieve data from large or unstructured datasets to inform LLM responses.\\n\\nBoth can be used together in a single application for complementary purposes!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754253100, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_f12167b370', usage=CompletionUsage(completion_tokens=748, prompt_tokens=19, total_tokens=767, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX-7MnFhVNoT"
   },
   "source": [
    "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
    "\n",
    "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB76LJrDVgbc"
   },
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-vmtUV7WVOLW"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"developer\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osXgB_5nVky_"
   },
   "source": [
    "### Testing Helper Functions\n",
    "\n",
    "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
    "\n",
    "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "4yRwAWvgWFNq",
    "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of large language model (LLM) applications, but they focus on different aspects and offer different capabilities. Here's a comparative overview:\n",
       "\n",
       "**LangChain:**\n",
       "\n",
       "- **Primary Focus:** Building composable, multi-step applications with LLMs, including chatbots, agents, and complex pipelines.\n",
       "- **Key Features:**\n",
       "  - Modular components such as prompts, memory, tools, and chains.\n",
       "  - Supports orchestration of multiple LLM calls and integrations.\n",
       "  - Built-in support for reasoning, chain-of-thought, and multi-turn conversations.\n",
       "  - Extensible framework for building agents that can interact with external APIs and tools.\n",
       "  - Supports various language models and deployment options.\n",
       "- **Use Cases:** Conversational AI, autonomous agents, complex workflows that combine LLMs with external tools.\n",
       "\n",
       "**LlamaIndex (GPT Index):**\n",
       "\n",
       "- **Primary Focus:** Efficiently indexing and querying large external data sources (like documents, files, or databases) using LLMs.\n",
       "- **Key Features:**\n",
       "  - Data ingestion pipelines to process and index various data formats.\n",
       "  - Rich indexing structures (like vectors, trees) for fast retrieval.\n",
       "  - Query engines that leverage LLMs to answer questions based on the indexed data.\n",
       "  - Designed to simplify building custom knowledge bases and document-aware applications.\n",
       "  - Supports multiple storage and retrieval backends.\n",
       "- **Use Cases:** Building question-answering systems over large document collections, knowledge bases, or internal corpora.\n",
       "\n",
       "---\n",
       "\n",
       "### Summary\n",
       "\n",
       "| Aspect               | **LangChain**                                   | **LlamaIndex (GPT Index)**                         |\n",
       "|----------------------|------------------------------------------------|--------------------------------------------------|\n",
       "| **Main Purpose**     | Building complex LLM applications and workflows | Indexing and querying large external data sources |\n",
       "| **Focus Area**       | Orchestrating multi-step interactions, agents | Efficient retrieval over big data collections  |\n",
       "| **Key Strengths**    | Modular chains, agents, tool integration      | Data ingestion, indexing, fast querying        |\n",
       "| **Use Cases**        | Chatbots, autonomous agents, AI workflows     | Knowledge bases, document QA, data-driven apps |\n",
       "\n",
       "**In essence:**  \n",
       "- Use **LangChain** if you're building applications that require orchestration, multi-step reasoning, or integration with multiple tools and APIs.  \n",
       "- Use **LlamaIndex** if your goal is to enable LLM-based querying and navigation over large document datasets or knowledge bases.\n",
       "\n",
       "They can also be complementary; for example, you might use LlamaIndex to manage and retrieve relevant documents and LangChain to orchestrate interactions, reasoning, and external tool calls based on that data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPs3ScS1WpoC"
   },
   "source": [
    "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
    "\n",
    "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
    "\n",
    ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "aSX2F3bDWYgy",
    "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Are you kidding me? At this point, I don't care if you give me a bucket of whatever! Just pick one — crushed or cubed — and make it quick! I'm starving and irate, and I need ice now!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFs56KVaXuEY"
   },
   "source": [
    "Let's try that same prompt again, but modify only our system prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "CGOlxfcFXxJ7",
    "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I think crushed ice is fantastic for a refreshing drink because it chills quickly and feels so satisfying to bite! Cubed ice, on the other hand, looks sleek and melts more slowly, making it perfect for sipping pretty much any beverage. Both have their charms—what's your preference?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkmjJd8zYQUK"
   },
   "source": [
    "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6b6z3CkYX9Y",
    "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-C0Zfn45RdQK60jdpnULuHh9c9srhh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is fantastic for a refreshing drink because it chills quickly and feels so satisfying to bite! Cubed ice, on the other hand, looks sleek and melts more slowly, making it perfect for sipping pretty much any beverage. Both have their charms—what's your preference?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754253163, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=57, prompt_tokens=30, total_tokens=87, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqMRJLbOYcwq"
   },
   "source": [
    "### Few-shot Prompting\n",
    "\n",
    "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
    "\n",
    "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
    "\n",
    "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "iLfNEH8Fcs6c",
    "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Here's a sentence using the words:\n",
       "\n",
       "\"During the whimsical tale, the stimple creature and the falbean flower became unlikely friends in the enchanted forest.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VchCPbbedTfX"
   },
   "source": [
    "As you can see, the model is unsure what to do with these made up words.\n",
    "\n",
    "Let's see if we can use the `assistant` role to show the model what these words mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "4InUN_ArZJpa",
    "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The stimple wrench worked perfectly with the falbean to securely fasten the bolts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0zn9-X2d23Z"
   },
   "source": [
    "As you can see, leveraging the `assistant` role makes for a stimple experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWUvXSWpeCs6"
   },
   "source": [
    "### Chain of Thought\n",
    "\n",
    "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions - but it can still benefit from a Chain of Thought Prompt to increase the reliability of the response!\n",
    "\n",
    "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "cwW0IgbfeTwP",
    "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze both options based on the current time and travel durations.\n",
       "\n",
       "**Current time:** 1PM local time (San Francisco), which is Pacific Daylight Time (PDT).  \n",
       "**Target time:** Before 7PM EDT.\n",
       "\n",
       "First, convert the target time to local time:\n",
       "\n",
       "- EDT (Eastern Daylight Time) is 3 hours ahead of PDT.\n",
       "\n",
       "So, **7PM EDT** is equivalent to:\n",
       "\n",
       "**4PM PDT**\n",
       "\n",
       "Billy needs to arrive **before 4PM PDT**.\n",
       "\n",
       "---\n",
       "\n",
       "### Option 1: Fly + Bus\n",
       "\n",
       "- Flight: 3 hours\n",
       "- Bus: 2 hours\n",
       "- Total time: 3 + 2 = 5 hours\n",
       "\n",
       "Starting at 1PM PDT:\n",
       "\n",
       "- After flight: 4PM PDT\n",
       "- After bus: 6PM PDT\n",
       "\n",
       "**Arrival time: 6PM PDT** — **before 4PM PDT target?** No, it arrives after the deadline.\n",
       "\n",
       "---\n",
       "\n",
       "### Option 2: Teleporter + Bus\n",
       "\n",
       "- Teleporter: 0 hours\n",
       "- Bus: 1 hour\n",
       "- Total time: 1 hour\n",
       "\n",
       "Starting at 1PM PDT:\n",
       "\n",
       "- Teleporter: remains at 1PM\n",
       "- Bus: 1 hour, so arrival at 2PM PDT\n",
       "\n",
       "**Arrival time: 2PM PDT** — well before the 4PM PDT deadline.\n",
       "\n",
       "---\n",
       "\n",
       "### **Conclusion:**\n",
       "\n",
       "**Yes, it does matter.**  \n",
       "Taking the teleporter + bus gets Billy home before 7PM EDT (which corresponds to 4PM PDT), while the fly + bus option arrives too late.\n",
       "\n",
       "**Billy should choose the teleporter + bus.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same prompt with a small modification - but this time include \"Let's think step by step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze the options step by step.\n",
       "\n",
       "### First, determine the current time in the local timezone and the deadline:\n",
       "\n",
       "- It's currently **1PM local time**.\n",
       "- Billy wants to arrive **before 7PM EDT**.\n",
       "\n",
       "### Important:  \n",
       "**San Francisco** is in **Pacific Daylight Time (PDT)** when daylight saving is active, which is **UTC-7**.  \n",
       "**Eastern Daylight Time (EDT)** is **UTC-4**.\n",
       "\n",
       "### Step 1: Convert current time to EDT:\n",
       "\n",
       "- PDT (San Francisco) is **UTC-7**.\n",
       "- 1PM PDT is equivalent to:\n",
       "\n",
       "  ```\n",
       "  1PM + (UTC-4) - (UTC-7) = 1PM + 3 hours = 4PM EDT\n",
       "  ```\n",
       "\n",
       "So, it is **4PM EDT** in San Francisco exactly right now.\n",
       "\n",
       "### Step 2: Calculate the deadline in local time:\n",
       "\n",
       "- Billy needs to arrive **before 7PM EDT**.\n",
       "- In his local time, the deadline is **7PM EDT**, which is **7PM EDT** — now, convert that to PDT:\n",
       "\n",
       "  ```\n",
       "  7PM EDT - 3 hours = 4PM PDT\n",
       "  ```\n",
       "\n",
       "Thus, **Billy must arrive in San Francisco by 4PM PDT**, which is **7PM EDT**.\n",
       "\n",
       "### Step 3: Assess the time remaining:\n",
       "\n",
       "- It's **current time**: 1PM PDT\n",
       "- **Deadline**: 4PM PDT\n",
       "\n",
       "Remaining time:\n",
       "\n",
       "```\n",
       "4PM - 1PM = 3 hours\n",
       "```\n",
       "\n",
       "Billy has **3 hours** to reach his destination **before 4PM PDT**.\n",
       "\n",
       "---\n",
       "\n",
       "### Step 4: Evaluate travel options:\n",
       "\n",
       "#### Option 1:\n",
       "- Fly (3 hours), then take a bus (2 hours).\n",
       "  \n",
       "  Total travel time: **3 + 2 = 5 hours**.\n",
       "\n",
       "- If Billy starts now (at 1PM PDT), he would arrive:\n",
       "\n",
       "  ```\n",
       "  1PM + 5 hours = 6PM PDT.\n",
       "  ```\n",
       "  \n",
       "  **6PM PDT** is **before** 4PM PDT?\n",
       "\n",
       "  **No**, 6PM PDT is **after** 4PM PDT.\n",
       "\n",
       "  **Wait**, this indicates he would **not** arrive before 4PM PDT.\n",
       "\n",
       "  **But hold on** — the calculation must be checked carefully:  \n",
       "  Arrival time if starting immediately:\n",
       "\n",
       "  ```\n",
       "  1PM + 3hrs(flying) + 2hrs(bus) = 6PM PDT\n",
       "  ```\n",
       "\n",
       "  Since the deadline is 4PM PDT, starting at 1PM PDT, the trip would **not** make it on time.\n",
       "\n",
       "  **Therefore**, for this trip to arrive before 4PM PDT, Billy must start **earlier**.\n",
       "\n",
       "  That is, he must start **by** 1PM PDT minus total travel time:\n",
       "\n",
       "  Since total travel time is 5 hours,\n",
       "\n",
       "  ```\n",
       "  Start time = 4PM PDT - 5 hours = **-1AM PDT**\n",
       "  ```\n",
       "\n",
       "  Which is in the past, so he **cannot** make it in time if starting now.\n",
       "\n",
       "#### Option 2:\n",
       "- Teleporter (0 hours), then bus (1 hour).\n",
       "\n",
       "  \n",
       "  Total travel time: **0 + 1 = 1 hour**.\n",
       "\n",
       "- Starting now (1PM PDT):\n",
       "\n",
       "  ```\n",
       "  1PM + 1 hour = 2PM PDT\n",
       "  ```\n",
       "\n",
       "  Arrival at 2PM PDT, which is **before** the 4PM PDT deadline.\n",
       "\n",
       "### **Conclusion:**\n",
       "\n",
       "- The teleporter + bus option guarantees arrival **by 2PM PDT**, well before the 4PM PDT deadline.\n",
       "- The flying + bus option would require starting **before 1PM PDT** to arrive by 4PM PDT, which is impossible since it's already 1PM.\n",
       "\n",
       "### **Final Answer:**\n",
       "\n",
       "**Yes, it does matter which option Billy selects.**\n",
       "\n",
       "- Taking the teleporter + bus ensures he arrives **on time**.\n",
       "- Taking the flight + bus **cannot** make it in time if he starts now.\n",
       "\n",
       "**Therefore, Billy should choose the teleporter + bus.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem + \"\\nLet's think step by step.\")\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFcrU-4pgRBS"
   },
   "source": [
    "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
    "\n",
    "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k9TKR1DhWI2"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
    "\n",
    "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/The-AI-Engineer-Challenge) repository!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rGI1nJeqeO_"
   },
   "source": [
    "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
